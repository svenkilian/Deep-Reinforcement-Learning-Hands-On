{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import datetime\n",
    "import pprint\n",
    "import gym\n",
    "import gym_anytrading\n",
    "import matplotlib.pyplot as plt\n",
    "from lib import data, environ\n",
    "from helpers import validation, environ\n",
    "from typing import List, Optional, Tuple, Any\n",
    "from tensorforce import Agent, Environment\n",
    "from tensorforce.agents import ConstantAgent\n",
    "from tensorforce.core.networks import AutoNetwork\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.core.layers import Dense, Gru\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-0.02325273  0.04247996  0.01807203 -0.03536022  0.0107492   0.01565327\n",
      "  0.13853578 -0.00494537  0.01483379  0.04487532]\n",
      "Reward: -0.04692015786483642\n",
      "Done: False\n",
      "Info: {'total_reward': -0.04692015786483642, 'total_profit': 1.0437865768315533, 'position': 0, 'offset': 94}\n",
      "Number of trading days in data: 730\n"
     ]
    }
   ],
   "source": [
    "env = environ.TradingEnv(\n",
    "    window_size=10,\n",
    "    commission_perc=0.001,\n",
    "    random_ofs_on_reset=True,\n",
    "    date_range=('2018-01-01', '2019-12-31')\n",
    ")\n",
    "env_val = environ.TradingEnv(\n",
    "    window_size=10,\n",
    "    commission_perc=0.001,\n",
    "    random_ofs_on_reset=False,\n",
    "    date_range=('2020-01-01', '2020-12-31')\n",
    ")\n",
    "\n",
    "obs = env.reset()\n",
    "obs, reward, done, info = env.step(0)\n",
    "print(f'Observation: {obs}')\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {done}\")\n",
    "print(f\"Info: {info}\")\n",
    "print(f'Number of trading days in data: {len(env.prices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensorforce environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: {'type': 'int', 'shape': (), 'num_values': 3}\n",
      "State space: {'type': 'float', 'shape': (10,), 'min_value': None, 'max_value': None}\n",
      "Initial state: [ 0.04664661 -0.03726715  0.04602445  0.00670416 -0.0180561  -0.01840975\n",
      "  0.01854566  0.05479704 -0.00421551  0.01492491]\n",
      "Initial state (validation): [ 0.         -0.03233423  0.05398306  0.00458934 -0.00058291  0.05243811\n",
      "  0.05111418 -0.01178317 -0.02694923  0.04395338]\n"
     ]
    }
   ],
   "source": [
    "environment = Environment.create(environment=env, max_episode_timesteps=100)\n",
    "environment_val = Environment.create(environment=env_val, max_episode_timesteps=1000)\n",
    "print(f'Action space: {environment.actions()}')\n",
    "print(f'State space: {environment.states()}')\n",
    "print(f'Initial state: {environment.reset()}')\n",
    "print(f'Initial state (validation): {environment_val.reset()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Tensorforce agent\n",
    "if False:\n",
    "    agent_ = Agent.create(\n",
    "        agent=\"dueling_dqn\",\n",
    "        max_episode_timesteps=1000,\n",
    "        environment=environment,\n",
    "        memory=100000,\n",
    "        # update=dict(unit=\"timesteps\", batch_size=32),\n",
    "        batch_size=32,\n",
    "        # optimizer=dict(type=\"adam\", learning_rate=3e-4),\n",
    "        # policy=dict(network=\"auto\"),\n",
    "        # objective=\"policy_gradient\",\n",
    "        start_updating=1e4,\n",
    "        # network=dict(network=[Gru(size=5, horizon=5, name='GRU_1'), Dense(size=3, name='Dense_1')]),\n",
    "        # network=dict(network=[dict(type='dense', size=32), dict(type='dense', size=3)]),\n",
    "        network='auto',\n",
    "        # reward_estimation=dict(horizon=4, discount=0.99),\n",
    "        discount=0.99,\n",
    "        target_sync_frequency=1e3,\n",
    "        config=dict(name=\"agent_007\"),\n",
    "        summarizer=dict(\n",
    "            directory=\"runs/summaries\",\n",
    "            # list of labels, or 'all'\n",
    "            summaries=[\"entropy\", \"kl-divergence\", \"loss\", \"reward\", \"update-norm\"],\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Tensorforce agent\n",
    "agent = Agent.create(\n",
    "    agent=\"a2c\",\n",
    "    environment=environment,\n",
    "    network=[\n",
    "        dict(\n",
    "            type=\"gru\",\n",
    "            size=64,\n",
    "            activation=\"tanh\",\n",
    "            horizon=1,\n",
    "            # dropout=0.1,\n",
    "            l2_regularization=0.01,\n",
    "        ),\n",
    "        # dict(\n",
    "        #     type=\"lstm\",\n",
    "        #     size=64,\n",
    "        #     activation=\"tanh\",\n",
    "        #     horizon=1,\n",
    "        #     dropout=0.1,\n",
    "        #     l2_regularization=0.01,\n",
    "        # ),\n",
    "        dict(type=\"dense\", size=16, activation=\"tanh\"),\n",
    "    ],\n",
    "    critic=[\n",
    "        dict(\n",
    "            type=\"gru\",\n",
    "            size=64,\n",
    "            activation=\"tanh\",\n",
    "            horizon=1,\n",
    "            # dropout=0.1,\n",
    "            l2_regularization=0.01,\n",
    "        ),\n",
    "        # dict(\n",
    "        #     type=\"lstm\",\n",
    "        #     size=64,\n",
    "        #     activation=\"tanh\",\n",
    "        #     horizon=1,\n",
    "        #     dropout=0.1,\n",
    "        #     l2_regularization=0.01,\n",
    "        # ),\n",
    "        dict(type=\"dense\", size=32, activation=\"tanh\"),\n",
    "    ],\n",
    "    # update=dict(unit=\"timesteps\", batch_size=32),\n",
    "    batch_size=32,\n",
    "    # objective=\"policy_gradient\",\n",
    "    # reward_estimation=dict(horizon=5),\n",
    "    # optimizer=dict(optimizer=\"rmsprop\", learning_rate=1e-3),\n",
    "    # memory=10000,  # Replay memory capacity\n",
    "    config=dict(name=\"agent_001\", device=\"gpu\"),\n",
    "    summarizer=dict(\n",
    "        directory=\"runs/summaries\",\n",
    "        # list of labels, or 'all'\n",
    "        # summaries=[\"entropy\", \"kl-divergence\", \"loss\", \"reward\", \"update-norm\"],\n",
    "        summaries=\"all\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# pprint.PrettyPrinter(indent=2).pprint(agent.get_specification())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_2022-03-22T16_11_32\n"
     ]
    }
   ],
   "source": [
    "latest_run_name = f'save_{datetime.datetime.now().isoformat(timespec=\"seconds\").replace(\":\", \"_\")}'\n",
    "print(latest_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e1a91b96b4fa38b4fd5875f54bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/400 [00:00, return=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_2022-03-22T16_11_32\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "runner.run(num_episodes=400, evaluation=True)\n",
    "agent.save('./saves', filename=f'{latest_run_name}', format='checkpoint')\n",
    "runner.close()\n",
    "print(f'{latest_run_name}')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_name = 'save_2022-03-22T15_37_15'\n",
    "agent = Agent.load('./saves', filename=f'{latest_run_name}', environment=environment_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = validation.Validator(env=environment_val, agent=agent, commission=None, num_episodes=1)\n",
    "res = validator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in res.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_val.environment.render_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0338d86274c1c6b1188772c0f97a7685f83d930028d9d1e47d4bafb465479f9d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('deep-rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
